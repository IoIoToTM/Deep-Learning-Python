{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-french.pkl\n",
      "go -- va\n",
      "run -- cours\n",
      "run -- courez\n",
      "fire -- au feu\n",
      "help -- a laide\n",
      "jump -- saute\n",
      "stop -- ca suffit\n",
      "stop -- stop\n",
      "stop -- arretetoi\n",
      "wait -- attends\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "filename = 'fra.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-french.pkl')\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print('%s -- %s' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154883, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-french.pkl')\n",
    "raw_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-french-both.pkl\n",
      "Saved: english-french-train.pkl\n",
      "Saved: english-french-test.pkl\n"
     ]
    }
   ],
   "source": [
    "#test = 51628\n",
    "#train = 103255\n",
    "#.30 split\n",
    "dataset = raw_dataset[:15000, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:13000], dataset[13000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-french-both.pkl')\n",
    "save_clean_data(train, 'english-french-train.pkl')\n",
    "save_clean_data(test, 'english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-french-both.pkl')\n",
    "train = load_clean_sentences('english-french-train.pkl')\n",
    "test = load_clean_sentences('english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2892\n",
      "English Max Length: 5\n",
      "french Vocabulary Size: 5778\n",
      "french Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare french tokenizer\n",
    "fre_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fre_vocab_size = len(fre_tokenizer.word_index) + 1\n",
    "fre_length = max_length(dataset[:, 1])\n",
    "print('french Vocabulary Size: %d' % fre_vocab_size)\n",
    "print('french Max Length: %d' % (fre_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fre_tokenizer, fre_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fre_tokenizer, fre_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           1479168   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2892)           743244    \n",
      "=================================================================\n",
      "Total params: 3,273,036\n",
      "Trainable params: 3,273,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(fre_vocab_size, eng_vocab_size, fre_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " - 15s - loss: 4.3234 - val_loss: 3.6822\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.68218, saving model to model.h5\n",
      "Epoch 2/100\n",
      " - 13s - loss: 3.5257 - val_loss: 3.5223\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.68218 to 3.52230, saving model to model.h5\n",
      "Epoch 3/100\n",
      " - 12s - loss: 3.3257 - val_loss: 3.3584\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.52230 to 3.35839, saving model to model.h5\n",
      "Epoch 4/100\n",
      " - 12s - loss: 3.1116 - val_loss: 3.2073\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.35839 to 3.20734, saving model to model.h5\n",
      "Epoch 5/100\n",
      " - 12s - loss: 2.9083 - val_loss: 3.0779\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.20734 to 3.07794, saving model to model.h5\n",
      "Epoch 6/100\n",
      " - 12s - loss: 2.7166 - val_loss: 2.9416\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.07794 to 2.94158, saving model to model.h5\n",
      "Epoch 7/100\n",
      " - 12s - loss: 2.5247 - val_loss: 2.8102\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.94158 to 2.81023, saving model to model.h5\n",
      "Epoch 8/100\n",
      " - 12s - loss: 2.3303 - val_loss: 2.6851\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.81023 to 2.68510, saving model to model.h5\n",
      "Epoch 9/100\n",
      " - 12s - loss: 2.1417 - val_loss: 2.5854\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.68510 to 2.58540, saving model to model.h5\n",
      "Epoch 10/100\n",
      " - 12s - loss: 1.9694 - val_loss: 2.4887\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.58540 to 2.48871, saving model to model.h5\n",
      "Epoch 11/100\n",
      " - 12s - loss: 1.8115 - val_loss: 2.4084\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.48871 to 2.40835, saving model to model.h5\n",
      "Epoch 12/100\n",
      " - 12s - loss: 1.6623 - val_loss: 2.3419\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.40835 to 2.34191, saving model to model.h5\n",
      "Epoch 13/100\n",
      " - 12s - loss: 1.5263 - val_loss: 2.2954\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.34191 to 2.29544, saving model to model.h5\n",
      "Epoch 14/100\n",
      " - 12s - loss: 1.4042 - val_loss: 2.2416\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.29544 to 2.24163, saving model to model.h5\n",
      "Epoch 15/100\n",
      " - 12s - loss: 1.2819 - val_loss: 2.1971\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.24163 to 2.19709, saving model to model.h5\n",
      "Epoch 16/100\n",
      " - 12s - loss: 1.1677 - val_loss: 2.1662\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.19709 to 2.16625, saving model to model.h5\n",
      "Epoch 17/100\n",
      " - 12s - loss: 1.0596 - val_loss: 2.1545\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.16625 to 2.15446, saving model to model.h5\n",
      "Epoch 18/100\n",
      " - 12s - loss: 0.9604 - val_loss: 2.1078\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.15446 to 2.10785, saving model to model.h5\n",
      "Epoch 19/100\n",
      " - 12s - loss: 0.8708 - val_loss: 2.0997\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.10785 to 2.09970, saving model to model.h5\n",
      "Epoch 20/100\n",
      " - 12s - loss: 0.7840 - val_loss: 2.0763\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.09970 to 2.07628, saving model to model.h5\n",
      "Epoch 21/100\n",
      " - 12s - loss: 0.7064 - val_loss: 2.0819\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.07628\n",
      "Epoch 22/100\n",
      " - 12s - loss: 0.6363 - val_loss: 2.0567\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.07628 to 2.05674, saving model to model.h5\n",
      "Epoch 23/100\n",
      " - 12s - loss: 0.5773 - val_loss: 2.0440\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.05674 to 2.04400, saving model to model.h5\n",
      "Epoch 24/100\n",
      " - 12s - loss: 0.5208 - val_loss: 2.0423\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.04400 to 2.04227, saving model to model.h5\n",
      "Epoch 25/100\n",
      " - 12s - loss: 0.4766 - val_loss: 2.0322\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.04227 to 2.03222, saving model to model.h5\n",
      "Epoch 26/100\n",
      " - 12s - loss: 0.4278 - val_loss: 2.0364\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.03222\n",
      "Epoch 27/100\n",
      " - 12s - loss: 0.3886 - val_loss: 2.0456\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.03222\n",
      "Epoch 28/100\n",
      " - 12s - loss: 0.3552 - val_loss: 2.0322\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.03222 to 2.03217, saving model to model.h5\n",
      "Epoch 29/100\n",
      " - 12s - loss: 0.3227 - val_loss: 2.0357\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.03217\n",
      "Epoch 30/100\n",
      " - 12s - loss: 0.2963 - val_loss: 2.0401\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.03217\n",
      "Epoch 31/100\n",
      " - 12s - loss: 0.2727 - val_loss: 2.0507\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.03217\n",
      "Epoch 32/100\n",
      " - 12s - loss: 0.2508 - val_loss: 2.0533\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 2.03217\n",
      "Epoch 33/100\n",
      " - 12s - loss: 0.2334 - val_loss: 2.0682\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 2.03217\n",
      "Epoch 34/100\n",
      " - 12s - loss: 0.2168 - val_loss: 2.0642\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.03217\n",
      "Epoch 35/100\n",
      " - 12s - loss: 0.2020 - val_loss: 2.0750\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 2.03217\n",
      "Epoch 36/100\n",
      " - 12s - loss: 0.1875 - val_loss: 2.0906\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.03217\n",
      "Epoch 37/100\n",
      " - 12s - loss: 0.1760 - val_loss: 2.1049\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.03217\n",
      "Epoch 38/100\n",
      " - 12s - loss: 0.1660 - val_loss: 2.1032\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.03217\n",
      "Epoch 39/100\n",
      " - 12s - loss: 0.1602 - val_loss: 2.1012\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 2.03217\n",
      "Epoch 40/100\n",
      " - 12s - loss: 0.1526 - val_loss: 2.1193\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.03217\n",
      "Epoch 41/100\n",
      " - 12s - loss: 0.1457 - val_loss: 2.1152\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.03217\n",
      "Epoch 42/100\n",
      " - 13s - loss: 0.1385 - val_loss: 2.1263\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 2.03217\n",
      "Epoch 43/100\n",
      " - 13s - loss: 0.1335 - val_loss: 2.1464\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.03217\n",
      "Epoch 44/100\n",
      " - 12s - loss: 0.1303 - val_loss: 2.1471\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.03217\n",
      "Epoch 45/100\n",
      " - 12s - loss: 0.1271 - val_loss: 2.1533\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.03217\n",
      "Epoch 46/100\n",
      " - 12s - loss: 0.1223 - val_loss: 2.1607\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.03217\n",
      "Epoch 47/100\n",
      " - 12s - loss: 0.1181 - val_loss: 2.1609\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.03217\n",
      "Epoch 48/100\n",
      " - 12s - loss: 0.1156 - val_loss: 2.1785\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.03217\n",
      "Epoch 49/100\n",
      " - 12s - loss: 0.1133 - val_loss: 2.1859\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.03217\n",
      "Epoch 50/100\n",
      " - 12s - loss: 0.1131 - val_loss: 2.1922\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.03217\n",
      "Epoch 51/100\n",
      " - 12s - loss: 0.1100 - val_loss: 2.1893\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.03217\n",
      "Epoch 52/100\n",
      " - 12s - loss: 0.1063 - val_loss: 2.1975\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.03217\n",
      "Epoch 53/100\n",
      " - 12s - loss: 0.1051 - val_loss: 2.2175\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.03217\n",
      "Epoch 54/100\n",
      " - 12s - loss: 0.1029 - val_loss: 2.2216\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.03217\n",
      "Epoch 55/100\n",
      " - 12s - loss: 0.1014 - val_loss: 2.2324\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.03217\n",
      "Epoch 56/100\n",
      " - 12s - loss: 0.1009 - val_loss: 2.2268\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 2.03217\n",
      "Epoch 57/100\n",
      " - 12s - loss: 0.0995 - val_loss: 2.2380\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.03217\n",
      "Epoch 58/100\n",
      " - 12s - loss: 0.0977 - val_loss: 2.2563\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.03217\n",
      "Epoch 59/100\n",
      " - 12s - loss: 0.0974 - val_loss: 2.2455\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.03217\n",
      "Epoch 60/100\n",
      " - 12s - loss: 0.0970 - val_loss: 2.2545\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.03217\n",
      "Epoch 61/100\n",
      " - 12s - loss: 0.0969 - val_loss: 2.2645\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.03217\n",
      "Epoch 62/100\n",
      " - 12s - loss: 0.0947 - val_loss: 2.2716\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.03217\n",
      "Epoch 63/100\n",
      " - 12s - loss: 0.0970 - val_loss: 2.2651\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.03217\n",
      "Epoch 64/100\n",
      " - 12s - loss: 0.0952 - val_loss: 2.2908\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.03217\n",
      "Epoch 65/100\n",
      " - 12s - loss: 0.0935 - val_loss: 2.3089\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.03217\n",
      "Epoch 66/100\n",
      " - 12s - loss: 0.0914 - val_loss: 2.2870\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.03217\n",
      "Epoch 67/100\n",
      " - 12s - loss: 0.0906 - val_loss: 2.2969\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.03217\n",
      "Epoch 68/100\n",
      " - 12s - loss: 0.0899 - val_loss: 2.3181\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.03217\n",
      "Epoch 69/100\n",
      " - 12s - loss: 0.0893 - val_loss: 2.3187\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.03217\n",
      "Epoch 70/100\n",
      " - 12s - loss: 0.0872 - val_loss: 2.3231\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.03217\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 12s - loss: 0.0889 - val_loss: 2.3174\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.03217\n",
      "Epoch 72/100\n",
      " - 12s - loss: 0.0885 - val_loss: 2.3304\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.03217\n",
      "Epoch 73/100\n",
      " - 12s - loss: 0.0902 - val_loss: 2.3300\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.03217\n",
      "Epoch 74/100\n",
      " - 12s - loss: 0.0884 - val_loss: 2.3377\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.03217\n",
      "Epoch 75/100\n",
      " - 12s - loss: 0.0866 - val_loss: 2.3468\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.03217\n",
      "Epoch 76/100\n",
      " - 12s - loss: 0.0879 - val_loss: 2.3553\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.03217\n",
      "Epoch 77/100\n",
      " - 12s - loss: 0.0878 - val_loss: 2.3567\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.03217\n",
      "Epoch 78/100\n",
      " - 12s - loss: 0.0864 - val_loss: 2.3425\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.03217\n",
      "Epoch 79/100\n",
      " - 15s - loss: 0.0866 - val_loss: 2.3533\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.03217\n",
      "Epoch 80/100\n",
      " - 14s - loss: 0.0847 - val_loss: 2.3757\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.03217\n",
      "Epoch 81/100\n",
      " - 13s - loss: 0.0849 - val_loss: 2.3735\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.03217\n",
      "Epoch 82/100\n",
      " - 13s - loss: 0.0842 - val_loss: 2.3675\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.03217\n",
      "Epoch 83/100\n",
      " - 13s - loss: 0.0830 - val_loss: 2.3723\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.03217\n",
      "Epoch 84/100\n",
      " - 13s - loss: 0.0838 - val_loss: 2.3829\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.03217\n",
      "Epoch 85/100\n",
      " - 13s - loss: 0.0830 - val_loss: 2.3848\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.03217\n",
      "Epoch 86/100\n",
      " - 13s - loss: 0.0828 - val_loss: 2.3889\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.03217\n",
      "Epoch 87/100\n",
      " - 13s - loss: 0.0842 - val_loss: 2.3776\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.03217\n",
      "Epoch 88/100\n",
      " - 15s - loss: 0.0835 - val_loss: 2.3999\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.03217\n",
      "Epoch 89/100\n",
      " - 14s - loss: 0.0826 - val_loss: 2.3998\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.03217\n",
      "Epoch 90/100\n",
      " - 13s - loss: 0.0829 - val_loss: 2.4057\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.03217\n",
      "Epoch 91/100\n",
      " - 13s - loss: 0.0840 - val_loss: 2.4199\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.03217\n",
      "Epoch 92/100\n",
      " - 13s - loss: 0.0830 - val_loss: 2.4250\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.03217\n",
      "Epoch 93/100\n",
      " - 14s - loss: 0.0821 - val_loss: 2.4028\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.03217\n",
      "Epoch 94/100\n",
      " - 13s - loss: 0.0812 - val_loss: 2.4262\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.03217\n",
      "Epoch 95/100\n",
      " - 13s - loss: 0.0811 - val_loss: 2.4289\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.03217\n",
      "Epoch 96/100\n",
      " - 13s - loss: 0.0812 - val_loss: 2.4364\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.03217\n",
      "Epoch 97/100\n",
      " - 13s - loss: 0.0814 - val_loss: 2.4340\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.03217\n",
      "Epoch 98/100\n",
      " - 13s - loss: 0.0830 - val_loss: 2.4512\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.03217\n",
      "Epoch 99/100\n",
      " - 13s - loss: 0.0817 - val_loss: 2.4391\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.03217\n",
      "Epoch 100/100\n",
      " - 13s - loss: 0.0802 - val_loss: 2.4469\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.03217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2661736d8d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "model.fit(trainX, trainY, \n",
    "          epochs=100, \n",
    "          batch_size=64, \n",
    "          validation_data=(testX, testY), \n",
    "          callbacks=[checkpoint], \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    " \n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    " \n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, \n",
    "                                     weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, \n",
    "                                     weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, \n",
    "                                     weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, \n",
    "                                     predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-french-both.pkl')\n",
    "train = load_clean_sentences('english-french-train.pkl')\n",
    "test = load_clean_sentences('english-french-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "# prepare freman tokenizer\n",
    "fre_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "fre_vocab_size = len(fre_tokenizer.word_index) + 1\n",
    "fre_length = max_length(dataset[:, 1])\n",
    "\n",
    "# prepare data\n",
    "trainX = encode_sequences(fre_tokenizer, fre_length, train[:, 1])\n",
    "testX = encode_sequences(fre_tokenizer, fre_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
